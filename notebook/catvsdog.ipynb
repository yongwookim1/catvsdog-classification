{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "27e5c5fd-1bc0-43cb-8b8a-0a0b748e2616",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from torchvision.models import resnet50\n",
    "\n",
    "import albumentations as A\n",
    "from albumentations.pytorch import ToTensorV2\n",
    "\n",
    "import numpy as np\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "41e6921b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "1523173f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10028\n",
      "../data/training_set/training_set/cats/cat.1.jpg\n"
     ]
    }
   ],
   "source": [
    "cat_train_dir = '../data/training_set/training_set/cats'\n",
    "dog_train_dir = '../data/training_set/training_set/dogs'\n",
    "cat_valid_dir = '../data/valid_set/valid_set/cats'\n",
    "dog_valid_dir = '../data/valid_set/valid_set/dogs'\n",
    "\n",
    "cat_train_filenames = sorted([os.path.join(cat_train_dir, f) for f in os.listdir(cat_train_dir) if not f.endswith('.DS_Store')])\n",
    "dog_train_filenames = sorted([os.path.join(dog_train_dir, f) for f in os.listdir(dog_train_dir) if not f.endswith('.DS_Store')])\n",
    "cat_valid_filenames = sorted([os.path.join(cat_valid_dir, f) for f in os.listdir(cat_valid_dir) if not f.endswith('.DS_Store')])\n",
    "dog_valid_filenames = sorted([os.path.join(dog_valid_dir, f) for f in os.listdir(dog_valid_dir) if not f.endswith('.DS_Store')])\n",
    "train_images_filenames = [*cat_train_filenames, *dog_train_filenames]\n",
    "valid_images_filenames = [*cat_valid_filenames, *dog_valid_filenames]\n",
    "images_filepaths = [*cat_train_filenames, *dog_train_filenames, *cat_valid_filenames, *dog_valid_filenames]\n",
    "\n",
    "print(len(images_filepaths))\n",
    "print(train_images_filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "142cc459",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_transforms = A.Compose([A.Resize(height=256, width=256),\n",
    "                              A.HorizontalFlip(p=0.5),\n",
    "                              A.CLAHE(always_apply=False, p=1.0, clip_limit=(4, 4), tile_grid_size=(8, 8)),\n",
    "                              A.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                          std=(0.5, 0.5, 0.5),\n",
    "                                          max_pixel_value=255.0,\n",
    "                                          always_apply=True),\n",
    "                              ToTensorV2(always_apply=True)])\n",
    "valid_transforms = A.Compose([A.Resize(height=256, width=256),\n",
    "                              A.CLAHE(always_apply=False, p=1.0, clip_limit=(4, 4), tile_grid_size=(8, 8)),\n",
    "                              A.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                          std=(0.5, 0.5, 0.5),\n",
    "                                          max_pixel_value=255.0,\n",
    "                                          always_apply=True),\n",
    "                              ToTensorV2(always_apply=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5782dd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatandDogDataset(Dataset):\n",
    "    def __init__(self, filenames, transforms):\n",
    "        self.filenames = filenames\n",
    "        self.transforms = transforms\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filenames[idx]\n",
    "        img = Image.open(img_path)\n",
    "        img = np.array(img)\n",
    "        transformed_img = self.transforms(image=img)['image']\n",
    "        label = 1 if 'cat' in img_path else 0\n",
    "        return transformed_img, label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "a162c911",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = CatandDogDataset(train_images_filenames, transforms=train_transforms)\n",
    "valid_dataset = CatandDogDataset(valid_images_filenames, transforms=valid_transforms)\n",
    "\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=64,\n",
    "                          num_workers=4,\n",
    "                          shuffle=True,\n",
    "                          drop_last=True)\n",
    "valid_loader = DataLoader(valid_dataset,\n",
    "                          batch_size=64,\n",
    "                          num_workers=4,\n",
    "                          shuffle=False,\n",
    "                          drop_last=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "5b142176",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([64, 3, 256, 256])\n",
      "tensor([0, 0, 1, 1, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1, 1, 0, 1, 0, 0, 1, 0, 0,\n",
      "        0, 0, 0, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 1,\n",
      "        0, 1, 1, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 1])\n"
     ]
    }
   ],
   "source": [
    "inputs, labels = next(iter(train_loader))\n",
    "print(inputs.size())\n",
    "print(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7033539a",
   "metadata": {},
   "outputs": [],
   "source": [
    "class ResNet50(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super(ResNet50, self).__init__()\n",
    "        self.model = resnet50(pretrained=False)\n",
    "        self.num_ftrs = self.model.fc.in_features\n",
    "        self.model.fc = nn.Linear(self.num_ftrs, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "34aacf19",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ResNet50(num_classes=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "dd673575",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(), lr=0.001, betas=(0.9, 0.999), eps=1e-08)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e5d45f57",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "print(device)\n",
    "\n",
    "model = model.to(device)\n",
    "criterion = criterion.to(device)\n",
    "\n",
    "set_seed(42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "1a16c2c2",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "----------------------------------------\n",
      "Epoch : 1/1\n",
      "train Loss : 0.6839 Acc : 0.6275\n",
      "valid Loss : 0.5798 Acc : 0.6639\n",
      "best Acc: 0.6639\n",
      "best Epoch : 1\n",
      "----------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "def train(model, train_loader, valid_loader, optimizer, criterion, num_epochs):\n",
    "    best_acc = 0.0\n",
    "    best_epoch = 1\n",
    "    for epoch in range(num_epochs):\n",
    "        print('-'*40)\n",
    "        print(f'Epoch : {epoch+1}/{num_epochs}')\n",
    "        epoch_loss = 0.0\n",
    "        epoch_corrects = 0\n",
    "        model.train()\n",
    "        for batch_in, batch_out in train_loader:\n",
    "            batch_in = batch_in.to(device)\n",
    "            batch_out = batch_out.to(device)\n",
    "            \n",
    "            y_pred = model(batch_in)\n",
    "            _, preds = torch.max(y_pred, 1)\n",
    "            \n",
    "            loss = criterion(y_pred, batch_out)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            epoch_loss += loss.item() * batch_in.size(0)\n",
    "            epoch_corrects += torch.sum(preds == batch_out.data)\n",
    "        \n",
    "        epoch_loss = epoch_loss / len(train_loader.dataset)\n",
    "        epoch_acc = epoch_corrects.double() / len(train_loader.dataset)\n",
    "        \n",
    "        print(f'train Loss : {epoch_loss:.4f} Acc : {epoch_acc:.4f}')\n",
    "        \n",
    "        epoch_loss = 0.0\n",
    "        epoch_corrects = 0\n",
    "        model.eval()\n",
    "        for batch_in, batch_out in valid_loader:\n",
    "            batch_in = batch_in.to(device)\n",
    "            batch_out = batch_out.to(device)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            with torch.no_grad():\n",
    "                y_pred = model(batch_in)\n",
    "                _, preds = torch.max(y_pred, 1)\n",
    "                \n",
    "                loss = criterion(y_pred, batch_out)\n",
    "                \n",
    "                epoch_loss += loss.item() * batch_in.size(0)\n",
    "                epoch_corrects += torch.sum(preds == batch_out.data)\n",
    "                \n",
    "        epoch_loss = epoch_loss / len(valid_loader.dataset)\n",
    "        epoch_acc = epoch_corrects.double() / len(valid_loader.dataset)\n",
    "        \n",
    "        if epoch_acc >= best_acc:\n",
    "            best_acc = epoch_acc\n",
    "            best_epoch = epoch + 1\n",
    "            torch.save(model, '../checkpoints/catvsdog.pt')\n",
    "            \n",
    "        print(f'valid Loss : {epoch_loss:.4f} Acc : {epoch_acc:.4f}')\n",
    "        print(f'best Acc: {best_acc:.4f}')\n",
    "        print(f'best Epoch : {best_epoch}')\n",
    "        print('-'*40)\n",
    "        print()\n",
    "\n",
    "train(model, train_loader, valid_loader, optimizer, criterion, num_epochs=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b455baa7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4\n",
      "../data/test_img/karina.jpg\n"
     ]
    }
   ],
   "source": [
    "test_dir = '../data/test_img'\n",
    "\n",
    "test_images_filenames = [os.path.join(test_dir, f) for f in os.listdir(test_dir)]\n",
    "\n",
    "print(len(test_images_filenames))\n",
    "print(test_images_filenames[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "839dcdc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_transforms = A.Compose([A.Resize(height=256, width=256),\n",
    "                             A.CLAHE(always_apply=False, p=1.0, clip_limit=(4, 4), tile_grid_size=(8, 8)),\n",
    "                             A.Normalize(mean=(0.5, 0.5, 0.5),\n",
    "                                          std=(0.5, 0.5, 0.5),\n",
    "                                          max_pixel_value=255.0,\n",
    "                                          always_apply=True),\n",
    "                             ToTensorV2(always_apply=True)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "4c861f14",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CatandDogInferenceDataset(Dataset):\n",
    "    def __init__(self, filenames, transforms):\n",
    "        self.filenames = filenames\n",
    "        self.transforms = transforms\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.filenames)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.filenames[idx]\n",
    "        img = Image.open(img_path)\n",
    "        img = np.array(img)\n",
    "        transformed_img = self.transforms(image=img)[\"image\"]\n",
    "        img_name = img_path.split(\"/\")[-1]\n",
    "        return transformed_img, img_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "5063d72b",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_dataset = CatandDogInferenceDataset(test_images_filenames, transforms=test_transforms)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                         batch_size=1,\n",
    "                         num_workers=4,\n",
    "                         shuffle=False,\n",
    "                         drop_last=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "dc8d7dcf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([1, 3, 256, 256])\n"
     ]
    }
   ],
   "source": [
    "inputs, filename = next(iter(test_loader))\n",
    "print(inputs.size())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "3bbeff89",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\"karina.jpg\" is a dog\n",
      "\"cat.jpeg\" is a dog\n",
      "\"dog.jpeg\" is a dog\n",
      "\"aroo.jpeg\" is a dog\n"
     ]
    }
   ],
   "source": [
    "def inference(model, test_loader):\n",
    "    model.eval()\n",
    "    for batch_in, img_name in test_loader:\n",
    "        batch_in = batch_in.to(device)\n",
    "\n",
    "        y_pred = model(batch_in)\n",
    "        y_pred = F.softmax(y_pred, dim=1)\n",
    "        _, preds = torch.max(y_pred, 1)\n",
    "\n",
    "        print(\n",
    "            f'\"{img_name[0]}\" is ' + \"a cat\"\n",
    "            if preds[0] == 1\n",
    "            else f'\"{img_name[0]}\" is ' + \"a dog\"\n",
    "        )\n",
    "\n",
    "model = torch.load('../checkpoints/catvsdog.pt')\n",
    "model.to(device)\n",
    "inference(model, test_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4547b405",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
